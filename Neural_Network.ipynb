{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Authors: \n",
    "        Ali Karimiafshar <karimiafsharali@gmail.com>\n",
    "        Vitor Freitas <vit1905@live.missouristate.edu>\n",
    "        Kadidia Kantao\n",
    "    Date:\n",
    "        12/09/2021\n",
    "    Dataset:\n",
    "        https://www.kaggle.com/c/digit-recognizer/data\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      4       0       0       0       0       0       0       0       0   \n",
       "4      0       0       0       0       0       0       0       0       0   \n",
       "5      0       0       0       0       0       0       0       0       0   \n",
       "6      7       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0         0         0         0   \n",
       "3       0  ...         0         0         0         0         0         0   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "5       0  ...         0         0         0         0         0         0   \n",
       "6       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "5         0         0         0         0  \n",
       "6         0         0         0         0  \n",
       "\n",
       "[7 rows x 785 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the data and display the first seven elements.\n",
    "data = pd.read_csv(\"train.csv\")\n",
    "data.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to numpy array\n",
    "data = np.array(data)\n",
    "\n",
    "# Divide the data in two groups for training (80%) and testing (20%).\n",
    "dataSampleSize, features = data.shape\n",
    "splitPoint = int(dataSampleSize*0.80)\n",
    "\n",
    "# Transpose the data so that each column corresponds with a new sample and each rwo with the pixel values.\n",
    "# Normalize the pixel values to a range [0, 1].\n",
    "data_train = data[:splitPoint].T\n",
    "X_train = data_train[1:] / 255\n",
    "Y_train = data_train[0]\n",
    "\n",
    "data_cross = data[splitPoint:].T\n",
    "X_cross = data_cross[1:] / 255\n",
    "Y_cross = data_cross[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetworkLayer:\n",
    "    def __init__(self, cur_num_neurons:int, next_num_neurons:int=10) -> None:\n",
    "        \"\"\" A Neural Network Layer that creates weights and biases arrays for the Neural Network \\\n",
    "            according to the number of neurons in the current layer and the next layer.\n",
    "\n",
    "        Args:\n",
    "            cur_num_neurons (int): Number of neurons on the current layer. Weights matrix columns.\n",
    "            next_num_neurons (int, optional): Number of neurons on the next layer. Weights matrix rows. Defaults to 10.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Initial range of randoms weights and biases is [-0.50, 0.50].\n",
    "        randRange = 0.5\n",
    "        np.random.seed(171317)\n",
    "        self.weights = np.random.uniform(-randRange, randRange, size=(next_num_neurons, cur_num_neurons))\n",
    "        self.biases = np.random.uniform(-randRange, randRange, size=(next_num_neurons, 1))\n",
    "        self.output = None\n",
    "        self.output_activated = None\n",
    "        \n",
    "        \n",
    "    def forward_prop(self, inputData:np.array) -> np.array:\n",
    "        \"\"\" Forward propagation. Performs matrix multiplication between weights and input data, then adds biases.\n",
    "\n",
    "        Args:\n",
    "            inputData (np.array): Input data or output data from the last neuron layer.\n",
    "\n",
    "        Returns:\n",
    "            np.array: The matrix multiplication product of weights and inputData plus the bias\n",
    "        \"\"\"\n",
    "        \n",
    "        self.output = self.weights.dot(inputData) + self.biases\n",
    "        return self.output\n",
    "        \n",
    "        \n",
    "    def activation_ReLU(self) -> np.array:\n",
    "        \"\"\" ReLU activation function.\n",
    "\n",
    "        Returns:\n",
    "            np.array: The forward propagation output, or zero, whichever is greater.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Element-wise maximum comparison.\n",
    "        self.output_activated = np.maximum(0, self.output)\n",
    "        return self.output_activated\n",
    "    \n",
    "    \n",
    "    def derivative_activation_ReLU(self) -> np.array:\n",
    "        \"\"\" ReLU derivative used for backward propagation. \n",
    "\n",
    "        Returns:\n",
    "            np.array: The derivative of the ReLU activation function, which is 1 when f>0, or 0 otherwise.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Bool will be converted to 0 or 1 when type casting.\n",
    "        return self.output_activated > 0\n",
    "    \n",
    "    \n",
    "    def activation_softmax(self) -> np.array:\n",
    "        \"\"\" Softmax activation function.\n",
    "\n",
    "        Returns:\n",
    "            np.array: The matrix prediction of the model based on the input data, weights, and biases of the Neural Network Layers. \n",
    "        \"\"\"\n",
    "        \n",
    "        # Element-wise exponential function divided by the sum of all exponential functions e^x.\n",
    "        self.output_activated = np.exp(self.output) / sum(np.exp(self.output))\n",
    "        return self.output_activated\n",
    "    \n",
    "    \n",
    "    def update_weights_biases(self, dW:np.array, dB:float, alpha:float) -> None:\n",
    "        \"\"\" Updates the weights and biases of the neural network layer based on the back propagation calculations.\n",
    "\n",
    "        Args:\n",
    "            dW (np.array): Change in weights.\n",
    "            dB (float): Change in biases.\n",
    "            alpha (float): Learning rate.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.weights -= alpha * dW\n",
    "        self.biases -= alpha * dB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitRecognizerNeuralNetwork:\n",
    "    def __init__(self, X:np.array, Y:np.array, SampleSize:int, iterations:int=500, alpha:float=0.10, isTraining=True, learned_wnb:Dict=None) -> None:\n",
    "        \"\"\" The Neural Network designed to classify handwriting digit samples found in the Kaggle dataset. \n",
    "\n",
    "        Args:\n",
    "            X (np.array): The training or testing data. Excludes the label.\n",
    "            Y (np.array): The data label.\n",
    "            SampleSize (int): Size of the dataset.\n",
    "            iterations (int, optional): Number of times the neural network is trained on the training dataset. Defaults to 500.\n",
    "            alpha (float, optional): Learning rate. Defaults to 0.10.\n",
    "            isTraining (bool, optional): If True, the network is a training network, otherwise a testing network.\n",
    "            learned_wnb (Dict, optional): The dictionary of learned weights and biases.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create the neuron layers\n",
    "        self.hiddenLayer = NeuralNetworkLayer(784,10)\n",
    "        self.outputLayer = NeuralNetworkLayer(10,10)\n",
    "        \n",
    "        # Initialize variables\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.size = SampleSize\n",
    "        self.iterations = iterations\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        # train the model if it is a training network, otherwise test cross validation data.\n",
    "        if isTraining:\n",
    "            self.gradient_descent()\n",
    "        else:\n",
    "            self.test_cross_data(learned_wnb)\n",
    "            \n",
    "\n",
    "    def forward_prop(self) -> None:\n",
    "        \"\"\" Calls the appropriate forward propagation and activation functions for each layer.\n",
    "        \"\"\"\n",
    "        \n",
    "        hiddenLayer_output = self.hiddenLayer.forward_prop(self.X)\n",
    "        hiddenLayer_activated = self.hiddenLayer.activation_ReLU()\n",
    "        \n",
    "        # Outputs of the hidden layer are the inputs to the output layer.\n",
    "        outputLayer_output = self.outputLayer.forward_prop(hiddenLayer_activated)\n",
    "        outputLayer_activated = self.outputLayer.activation_softmax()\n",
    "                \n",
    "\n",
    "    def get_actual_label(self, Y:np.array) -> np.array:\n",
    "        \"\"\" Generates an array of zeros with a 1 in the index of the actual label of the training data. \\\n",
    "            This is used to calculate the deviation of the network prediction from the actual label.\n",
    "\n",
    "        Args:\n",
    "            Y (np.array): The label array of training data\n",
    "\n",
    "        Returns:\n",
    "            np.array: array of zeros with only one value of 1 at the index of the label Y.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Generate array of zeros\n",
    "        actual_y = np.zeros((Y.size, 10))\n",
    "        \n",
    "        # for each row (data), replace the column Y index (label index), with a 1.\n",
    "        actual_y[[i for i in range(Y.size)], Y] = 1\n",
    "        \n",
    "        # Transpose and return the array so that each row corresponds with the value of the label.\n",
    "        # For example, if the label of the first row is 4, return [0, 0, 0, 0, 1, 0, 0, 0, 0, 0] as the columns of the first row.\n",
    "        # Then transpose and return.\n",
    "        return actual_y.T\n",
    "    \n",
    "    \n",
    "    def backward_prop(self) -> None:\n",
    "        \"\"\" Performs the back propagation algorithm to calculate how much weights and biases of each layer need to be changed by.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Array of zeroes with one 1 at the index of the actual label of the data.\n",
    "        actual_y = self.get_actual_label(self.Y)\n",
    "        \n",
    "        # Determine how the output layer weights and biases need to be updated.\n",
    "        dZ_outputLayer = self.outputLayer.output_activated - actual_y\n",
    "        dW_outputLayer = 1 / self.size * dZ_outputLayer.dot(self.hiddenLayer.output_activated.T)\n",
    "        db_outputLayer = 1 / self.size * np.sum(dZ_outputLayer)\n",
    "        \n",
    "        # Determine how the hidden layer weights and biases need to be updated.\n",
    "        dZ_hiddenLayer = self.outputLayer.weights.T.dot(dZ_outputLayer) * self.hiddenLayer.derivative_activation_ReLU()\n",
    "        dW_hiddenLayer = 1 / self.size * dZ_hiddenLayer.dot(self.X.T)\n",
    "        db_hiddenLayer = 1 / self.size * np.sum(dZ_hiddenLayer)\n",
    "        \n",
    "        # Updates the weights and biases of each layer.\n",
    "        self.outputLayer.update_weights_biases(dW_outputLayer, db_outputLayer, self.alpha)\n",
    "        self.hiddenLayer.update_weights_biases(dW_hiddenLayer, db_hiddenLayer, self.alpha)\n",
    "        \n",
    "        \n",
    "    def get_predictions(self, outputLayer_output:np.array) -> np.array:\n",
    "        \"\"\" calculates the prediction of the neural network based on the maximum of the output layer.\n",
    "\n",
    "        Args:\n",
    "            outputLayer_output (np.array): values of the output layer, whose maximum would be the network prediction.\n",
    "\n",
    "        Returns:\n",
    "            np.array: index of the maximum value of the output layer, which corresponds with the digit predicted in range [0, 9]. \n",
    "        \"\"\"\n",
    "        \n",
    "        return np.argmax(outputLayer_output, 0)\n",
    "\n",
    "\n",
    "    def get_accuracy(self, predictions:np.array, Y:np.array, numDigitsToShow:int=17) -> float:\n",
    "        \"\"\" Calculates the accuracy of the model.\n",
    "\n",
    "        Args:\n",
    "            predictions (np.array): array of the network predictions based on the activation function of the output layer.\n",
    "            Y (np.array): array of the actual data labels.\n",
    "            numDigitsToShow (int, optional): Number of first elements of prediction and label arrays to discplay. Defaults to 17.\n",
    "\n",
    "        Returns:\n",
    "            float: Accuracy of the model out of 100%.\n",
    "        \"\"\"\n",
    "        \n",
    "        accuracy = np.sum(predictions == Y) / Y.size\n",
    "        \n",
    "        if numDigitsToShow:\n",
    "            print(f\"Network predictions:\\t{predictions[:numDigitsToShow]}\\nActual labels:\\t\\t{Y[:numDigitsToShow]}\")\n",
    "        \n",
    "        return accuracy\n",
    "    \n",
    "    \n",
    "    def gradient_descent(self) -> None:\n",
    "        \"\"\" Performs the gradient decent a number of iterations to train the model. \\\n",
    "            Calls forward_prop, and backward_prop, then displays updated accuracy of the model with every 10 iterations.\n",
    "        \"\"\"\n",
    "        \n",
    "        for i in range(1, self.iterations+1):\n",
    "            self.forward_prop()\n",
    "            self.backward_prop()\n",
    "            \n",
    "            # For every 10 training iterations, display updates.\n",
    "            if (i % 10) == 0:\n",
    "                print(f\"iteration: {i}\")\n",
    "                self.display_updates()\n",
    "        \n",
    "        # After the training iterations, store the weights and biases in tunes dictionary.\n",
    "        self.tunes = {\"hiddenLayer_weights\":self.hiddenLayer.weights,\n",
    "                      \"hiddenLayer_biases\":self.hiddenLayer.biases,\n",
    "                      \"outputLayer_weights\":self.outputLayer.weights,\n",
    "                      \"outputLayer_biases\":self.outputLayer.biases}\n",
    "        \n",
    "    \n",
    "    def display_updates(self) -> None:\n",
    "        \"\"\" Displays updated accuracy of the model.\n",
    "        \"\"\"\n",
    "        \n",
    "        neural_network_output = self.outputLayer.output\n",
    "        self.predictions = self.get_predictions(neural_network_output)\n",
    "        accuracy = self.get_accuracy(self.predictions, self.Y)\n",
    "        print(f\"Accuracy:\\t{accuracy*100:.2f}%\")\n",
    "    \n",
    "    \n",
    "    def load_weights_and_biases(self, wnb_dict:Dict) -> None:\n",
    "        \"\"\" Loads learned weights and biases into the model for testing unexamined sample data.\n",
    "\n",
    "        Args:\n",
    "            wnb_dict (Dict): Dictionary of the weights and biases.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.hiddenLayer.weights = wnb_dict[\"hiddenLayer_weights\"]\n",
    "        self.hiddenLayer.biases = wnb_dict[\"hiddenLayer_biases\"]\n",
    "        self.outputLayer.weights = wnb_dict[\"outputLayer_weights\"]\n",
    "        self.outputLayer.biases = wnb_dict[\"outputLayer_biases\"]\n",
    "    \n",
    "    \n",
    "    def test_cross_data(self, learned_wnb:Dict) -> None:\n",
    "        \"\"\" Classifies cross validation data based on the learned weights and biases. \\\n",
    "            Then displays the accuracy of the model.\n",
    "\n",
    "        Args:\n",
    "            learned_wnb (Dict): learned weights and biases to be loaded into the network.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.load_weights_and_biases(learned_wnb)\n",
    "        self.forward_prop()\n",
    "        self.display_updates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 10\n",
      "Network predictions:\t[1 8 4 2 2 2 6 1 2 2 8 8 1 5 2 5 1]\n",
      "Actual labels:\t\t[1 0 1 4 0 0 7 3 5 3 8 9 1 3 3 1 2]\n",
      "Accuracy:\t15.70%\n",
      "iteration: 20\n",
      "Network predictions:\t[1 2 1 2 2 2 6 9 0 2 8 8 1 5 2 5 1]\n",
      "Actual labels:\t\t[1 0 1 4 0 0 7 3 5 3 8 9 1 3 3 1 2]\n",
      "Accuracy:\t23.13%\n",
      "iteration: 30\n",
      "Network predictions:\t[1 2 1 2 2 2 6 9 0 2 9 9 1 5 2 5 1]\n",
      "Actual labels:\t\t[1 0 1 4 0 0 7 3 5 3 8 9 1 3 3 1 2]\n",
      "Accuracy:\t27.77%\n",
      "iteration: 40\n",
      "Network predictions:\t[1 2 1 2 2 2 6 8 0 2 8 9 1 5 2 5 1]\n",
      "Actual labels:\t\t[1 0 1 4 0 0 7 3 5 3 8 9 1 3 3 1 2]\n",
      "Accuracy:\t31.56%\n",
      "iteration: 50\n",
      "Network predictions:\t[1 0 1 2 2 2 6 4 0 0 8 9 1 1 2 1 1]\n",
      "Actual labels:\t\t[1 0 1 4 0 0 7 3 5 3 8 9 1 3 3 1 2]\n",
      "Accuracy:\t34.91%\n",
      "iteration: 60\n",
      "Network predictions:\t[1 0 1 2 2 2 6 4 0 2 8 9 1 1 2 1 1]\n",
      "Actual labels:\t\t[1 0 1 4 0 0 7 3 5 3 8 9 1 3 3 1 2]\n",
      "Accuracy:\t38.29%\n",
      "iteration: 70\n",
      "Network predictions:\t[1 0 1 4 0 2 7 4 0 2 8 9 1 1 2 1 1]\n",
      "Actual labels:\t\t[1 0 1 4 0 0 7 3 5 3 8 9 1 3 3 1 2]\n",
      "Accuracy:\t41.51%\n",
      "iteration: 80\n",
      "Network predictions:\t[1 0 1 4 0 2 7 4 8 2 8 9 1 1 2 1 1]\n",
      "Actual labels:\t\t[1 0 1 4 0 0 7 3 5 3 8 9 1 3 3 1 2]\n",
      "Accuracy:\t44.68%\n",
      "iteration: 90\n",
      "Network predictions:\t[1 0 1 4 0 2 7 4 8 2 8 9 1 3 2 1 3]\n",
      "Actual labels:\t\t[1 0 1 4 0 0 7 3 5 3 8 9 1 3 3 1 2]\n",
      "Accuracy:\t48.34%\n",
      "iteration: 100\n",
      "Network predictions:\t[1 0 1 4 0 2 7 4 8 2 7 9 1 3 2 1 3]\n",
      "Actual labels:\t\t[1 0 1 4 0 0 7 3 5 3 8 9 1 3 3 1 2]\n",
      "Accuracy:\t51.64%\n",
      "iteration: 110\n",
      "Network predictions:\t[1 0 1 4 0 2 7 4 8 2 7 9 1 3 2 1 3]\n",
      "Actual labels:\t\t[1 0 1 4 0 0 7 3 5 3 8 9 1 3 3 1 2]\n",
      "Accuracy:\t54.62%\n",
      "iteration: 120\n",
      "Network predictions:\t[1 0 1 4 0 2 7 8 8 2 7 9 1 3 2 1 3]\n",
      "Actual labels:\t\t[1 0 1 4 0 0 7 3 5 3 8 9 1 3 3 1 2]\n",
      "Accuracy:\t57.45%\n",
      "iteration: 130\n",
      "Network predictions:\t[1 0 1 4 0 2 7 8 8 2 7 9 1 3 3 1 3]\n",
      "Actual labels:\t\t[1 0 1 4 0 0 7 3 5 3 8 9 1 3 3 1 2]\n",
      "Accuracy:\t60.00%\n",
      "iteration: 140\n",
      "Network predictions:\t[1 0 1 4 0 8 7 8 8 3 7 9 1 3 3 1 3]\n",
      "Actual labels:\t\t[1 0 1 4 0 0 7 3 5 3 8 9 1 3 3 1 2]\n",
      "Accuracy:\t62.22%\n",
      "iteration: 150\n",
      "Network predictions:\t[1 0 1 4 0 8 7 8 5 3 7 9 1 3 3 1 3]\n",
      "Actual labels:\t\t[1 0 1 4 0 0 7 3 5 3 8 9 1 3 3 1 2]\n",
      "Accuracy:\t64.11%\n",
      "iteration: 160\n",
      "Network predictions:\t[1 0 1 4 0 8 7 8 5 3 7 9 1 3 3 1 3]\n",
      "Actual labels:\t\t[1 0 1 4 0 0 7 3 5 3 8 9 1 3 3 1 2]\n",
      "Accuracy:\t65.82%\n",
      "iteration: 170\n",
      "Network predictions:\t[1 0 1 4 0 8 7 8 5 3 7 9 1 3 3 1 3]\n",
      "Actual labels:\t\t[1 0 1 4 0 0 7 3 5 3 8 9 1 3 3 1 2]\n",
      "Accuracy:\t67.34%\n",
      "iteration: 180\n",
      "Network predictions:\t[1 0 1 4 0 8 7 8 5 3 7 9 1 3 3 1 3]\n",
      "Actual labels:\t\t[1 0 1 4 0 0 7 3 5 3 8 9 1 3 3 1 2]\n",
      "Accuracy:\t68.63%\n",
      "iteration: 190\n",
      "Network predictions:\t[1 0 1 4 0 8 7 8 5 3 7 9 1 3 3 1 3]\n",
      "Actual labels:\t\t[1 0 1 4 0 0 7 3 5 3 8 9 1 3 3 1 2]\n",
      "Accuracy:\t69.91%\n",
      "iteration: 200\n",
      "Network predictions:\t[1 0 1 4 0 8 7 8 5 3 7 9 1 3 3 1 3]\n",
      "Actual labels:\t\t[1 0 1 4 0 0 7 3 5 3 8 9 1 3 3 1 2]\n",
      "Accuracy:\t70.99%\n",
      "iteration: 210\n",
      "Network predictions:\t[1 0 1 4 0 8 7 8 5 3 7 9 1 3 3 1 3]\n",
      "Actual labels:\t\t[1 0 1 4 0 0 7 3 5 3 8 9 1 3 3 1 2]\n",
      "Accuracy:\t72.01%\n",
      "iteration: 220\n",
      "Network predictions:\t[1 0 1 4 0 8 7 3 5 3 7 9 1 3 3 1 3]\n",
      "Actual labels:\t\t[1 0 1 4 0 0 7 3 5 3 8 9 1 3 3 1 2]\n",
      "Accuracy:\t72.96%\n",
      "iteration: 230\n",
      "Network predictions:\t[1 0 1 4 0 8 7 3 5 3 8 9 1 3 3 1 3]\n",
      "Actual labels:\t\t[1 0 1 4 0 0 7 3 5 3 8 9 1 3 3 1 2]\n",
      "Accuracy:\t73.92%\n",
      "iteration: 240\n",
      "Network predictions:\t[1 0 1 4 0 8 7 3 5 3 8 9 1 3 3 1 3]\n",
      "Actual labels:\t\t[1 0 1 4 0 0 7 3 5 3 8 9 1 3 3 1 2]\n",
      "Accuracy:\t74.69%\n",
      "iteration: 250\n",
      "Network predictions:\t[1 0 1 4 0 8 7 3 5 3 8 9 1 3 3 1 3]\n",
      "Actual labels:\t\t[1 0 1 4 0 0 7 3 5 3 8 9 1 3 3 1 2]\n",
      "Accuracy:\t75.46%\n",
      "iteration: 260\n",
      "Network predictions:\t[1 0 1 4 0 8 7 3 5 3 8 9 1 3 3 1 3]\n",
      "Actual labels:\t\t[1 0 1 4 0 0 7 3 5 3 8 9 1 3 3 1 2]\n",
      "Accuracy:\t76.22%\n",
      "iteration: 270\n",
      "Network predictions:\t[1 0 1 4 0 8 7 3 5 3 8 9 1 3 3 1 3]\n",
      "Actual labels:\t\t[1 0 1 4 0 0 7 3 5 3 8 9 1 3 3 1 2]\n",
      "Accuracy:\t76.89%\n",
      "iteration: 280\n",
      "Network predictions:\t[1 0 1 4 0 0 7 3 5 3 8 9 1 3 3 1 3]\n",
      "Actual labels:\t\t[1 0 1 4 0 0 7 3 5 3 8 9 1 3 3 1 2]\n",
      "Accuracy:\t77.50%\n",
      "iteration: 290\n",
      "Network predictions:\t[1 0 1 4 0 0 7 3 5 3 8 9 1 3 3 1 3]\n",
      "Actual labels:\t\t[1 0 1 4 0 0 7 3 5 3 8 9 1 3 3 1 2]\n",
      "Accuracy:\t78.08%\n",
      "iteration: 300\n",
      "Network predictions:\t[1 0 1 4 0 0 7 3 5 3 8 9 1 3 3 1 3]\n",
      "Actual labels:\t\t[1 0 1 4 0 0 7 3 5 3 8 9 1 3 3 1 2]\n",
      "Accuracy:\t78.61%\n",
      "iteration: 310\n",
      "Network predictions:\t[1 0 1 4 0 0 7 3 5 3 8 9 1 3 3 1 3]\n",
      "Actual labels:\t\t[1 0 1 4 0 0 7 3 5 3 8 9 1 3 3 1 2]\n",
      "Accuracy:\t79.14%\n",
      "iteration: 320\n",
      "Network predictions:\t[1 0 1 4 0 0 7 3 5 3 8 9 1 3 3 1 3]\n",
      "Actual labels:\t\t[1 0 1 4 0 0 7 3 5 3 8 9 1 3 3 1 2]\n",
      "Accuracy:\t79.56%\n",
      "iteration: 330\n",
      "Network predictions:\t[1 0 1 4 0 0 7 3 5 3 8 9 1 3 3 1 3]\n",
      "Actual labels:\t\t[1 0 1 4 0 0 7 3 5 3 8 9 1 3 3 1 2]\n",
      "Accuracy:\t79.96%\n",
      "iteration: 340\n",
      "Network predictions:\t[1 0 1 4 0 0 7 3 5 3 8 9 1 3 3 1 3]\n",
      "Actual labels:\t\t[1 0 1 4 0 0 7 3 5 3 8 9 1 3 3 1 2]\n",
      "Accuracy:\t80.29%\n",
      "iteration: 350\n",
      "Network predictions:\t[1 0 1 4 0 0 7 3 5 3 8 9 1 3 3 1 3]\n",
      "Actual labels:\t\t[1 0 1 4 0 0 7 3 5 3 8 9 1 3 3 1 2]\n",
      "Accuracy:\t80.67%\n",
      "iteration: 360\n",
      "Network predictions:\t[1 0 1 4 0 0 7 3 5 3 8 9 1 3 3 1 3]\n",
      "Actual labels:\t\t[1 0 1 4 0 0 7 3 5 3 8 9 1 3 3 1 2]\n",
      "Accuracy:\t80.96%\n",
      "iteration: 370\n",
      "Network predictions:\t[1 0 1 4 0 0 7 3 5 3 8 9 1 3 3 1 3]\n",
      "Actual labels:\t\t[1 0 1 4 0 0 7 3 5 3 8 9 1 3 3 1 2]\n",
      "Accuracy:\t81.28%\n",
      "iteration: 380\n",
      "Network predictions:\t[1 0 1 6 0 0 7 3 5 3 8 9 1 3 3 1 3]\n",
      "Actual labels:\t\t[1 0 1 4 0 0 7 3 5 3 8 9 1 3 3 1 2]\n",
      "Accuracy:\t81.57%\n",
      "iteration: 390\n",
      "Network predictions:\t[1 0 1 6 0 0 7 3 5 3 8 9 1 3 3 1 3]\n",
      "Actual labels:\t\t[1 0 1 4 0 0 7 3 5 3 8 9 1 3 3 1 2]\n",
      "Accuracy:\t81.81%\n",
      "iteration: 400\n",
      "Network predictions:\t[1 0 1 6 0 0 7 3 5 3 8 9 1 3 3 1 3]\n",
      "Actual labels:\t\t[1 0 1 4 0 0 7 3 5 3 8 9 1 3 3 1 2]\n",
      "Accuracy:\t82.09%\n",
      "iteration: 410\n",
      "Network predictions:\t[1 0 1 6 0 0 7 3 5 3 8 9 1 3 3 1 8]\n",
      "Actual labels:\t\t[1 0 1 4 0 0 7 3 5 3 8 9 1 3 3 1 2]\n",
      "Accuracy:\t82.36%\n",
      "iteration: 420\n",
      "Network predictions:\t[1 0 1 6 0 0 7 3 5 3 8 9 1 3 3 1 8]\n",
      "Actual labels:\t\t[1 0 1 4 0 0 7 3 5 3 8 9 1 3 3 1 2]\n",
      "Accuracy:\t82.66%\n",
      "iteration: 430\n",
      "Network predictions:\t[1 0 1 6 0 0 7 3 5 3 8 9 1 3 3 1 8]\n",
      "Actual labels:\t\t[1 0 1 4 0 0 7 3 5 3 8 9 1 3 3 1 2]\n",
      "Accuracy:\t82.84%\n",
      "iteration: 440\n",
      "Network predictions:\t[1 0 1 6 0 0 7 3 5 3 8 9 1 3 3 1 8]\n",
      "Actual labels:\t\t[1 0 1 4 0 0 7 3 5 3 8 9 1 3 3 1 2]\n",
      "Accuracy:\t83.06%\n",
      "iteration: 450\n",
      "Network predictions:\t[1 0 1 6 0 0 7 3 5 3 8 9 1 3 3 1 8]\n",
      "Actual labels:\t\t[1 0 1 4 0 0 7 3 5 3 8 9 1 3 3 1 2]\n",
      "Accuracy:\t83.27%\n",
      "iteration: 460\n",
      "Network predictions:\t[1 0 1 6 0 0 7 3 5 3 8 9 1 3 3 1 8]\n",
      "Actual labels:\t\t[1 0 1 4 0 0 7 3 5 3 8 9 1 3 3 1 2]\n",
      "Accuracy:\t83.44%\n",
      "iteration: 470\n",
      "Network predictions:\t[1 0 1 6 0 0 7 3 5 3 8 9 1 3 3 1 8]\n",
      "Actual labels:\t\t[1 0 1 4 0 0 7 3 5 3 8 9 1 3 3 1 2]\n",
      "Accuracy:\t83.64%\n",
      "iteration: 480\n",
      "Network predictions:\t[1 0 1 6 0 0 7 3 5 3 8 9 1 3 3 1 8]\n",
      "Actual labels:\t\t[1 0 1 4 0 0 7 3 5 3 8 9 1 3 3 1 2]\n",
      "Accuracy:\t83.79%\n",
      "iteration: 490\n",
      "Network predictions:\t[1 0 1 6 0 0 7 3 5 3 8 9 1 3 3 1 8]\n",
      "Actual labels:\t\t[1 0 1 4 0 0 7 3 5 3 8 9 1 3 3 1 2]\n",
      "Accuracy:\t83.99%\n",
      "iteration: 500\n",
      "Network predictions:\t[1 0 1 6 0 0 7 3 5 3 8 9 1 3 3 1 8]\n",
      "Actual labels:\t\t[1 0 1 4 0 0 7 3 5 3 8 9 1 3 3 1 2]\n",
      "Accuracy:\t84.17%\n"
     ]
    }
   ],
   "source": [
    "digit_neural_network = DigitRecognizerNeuralNetwork(X_train, Y_train, dataSampleSize, iterations=500, alpha=0.10)\n",
    "learned_weights_and_biases = digit_neural_network.tunes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network predictions:\t[0 7 7 2 2 6 0 7 3 5 2 0 7 7 6 9 2]\n",
      "Actual labels:\t\t[0 7 7 2 2 6 5 7 8 5 3 0 2 7 6 9 2]\n",
      "Accuracy:\t83.77%\n"
     ]
    }
   ],
   "source": [
    "testing_digit_network = DigitRecognizerNeuralNetwork(X_cross, Y_cross, dataSampleSize, 0, 0, isTraining=False, learned_wnb=learned_weights_and_biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual label:\t8\n",
      "pred label:\t3\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ix = 8\n",
    "Y_cross[ix]\n",
    "pred = testing_digit_network.predictions\n",
    "\n",
    "def display_sample(X, ix, predictions):\n",
    "    pixels = X[:,ix].reshape(28, 28)\n",
    "    print(f\"Actual label:\\t{Y_cross[ix]}\\npred label:\\t{predictions[ix]}\\n\")\n",
    "    plt.imshow(pixels, plt.cm.gray_r)\n",
    "\n",
    "\n",
    "display_sample(X_cross, ix, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for n in range(0, 7):\n",
    "#     # n = 1\n",
    "#     # print(X_train[:,n].shape)\n",
    "#     digit = X_train[:,n].reshape(28, 28)\n",
    "#     plt.figure()\n",
    "#     plt.imshow(digit, plt.cm.gray_r)\n",
    "#     # out = a.NNoutput[:,0]\n",
    "#     Y_train[n]\n",
    "#     # pred = a.get_predictions(a.NNoutput)\n",
    "#     pred = testing_digit_network.predictions\n",
    "#     print(f\"Label: {Y_train[n]}\\tPrediction:{pred[n]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'out' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3324/3474855571.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdisplay_sample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'out' is not defined"
     ]
    }
   ],
   "source": [
    "display_sample(9, X_train, Y_train, out)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
